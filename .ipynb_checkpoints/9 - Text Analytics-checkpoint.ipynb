{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Text Analytics\n",
    "\n",
    "Text analysis, sometimes called text analytics, refers to the representation, processing, and modeling of textual data to derive useful insights.\n",
    "\n",
    "In general, text analysis is concerned with a *corpus* of documents. These could be sentences in a paragraph, chapters in a book, or indeed books in a corpus. We can typically break the process of analysing text into three steps:\n",
    "\n",
    "1. **Parsing** is the process of imposing some structure on unstructured text. For example, we could break a raw HTML file into paragraphs, and each paragraph into indivdual words.\n",
    "\n",
    "2. **Search and Retrieval** is the identification of documents containing certain *key terms* deemed relevant to the analysis.\n",
    "\n",
    "3. **Text Mining** uses the terms and indices of the previous steps to discover patterns and insights.\n",
    "\n",
    "Text data is incompatible with the models we have discussed so far because the models require numeric values. For example, we don't have a direct numeric distance between the words \"hello\" and \"friend\", so the k-means clustering algorithm cannot be applied to raw text data.\n",
    "\n",
    "Moreover, sometimes single data points such as a tweet, facebook post etc will contain a large number of distinct words. So we need a way to convert the text data into a flexible numeric representation. This representation should tell us which words occured and how important their appearance is to the meaning of a document. This should be related to how many times the word appears, and how much information is contained in the word.\n",
    "\n",
    "There are many ways to represent text numerically. The simplest way is known as Bag-of-Words (BoW) representation.\n",
    "\n",
    "\n",
    "## Bag-of-Words\n",
    "\n",
    "The bag-of-words model is a simple method of transforming strings of text into a numeric representation. BoW treats each word as a feature and the value of the feature is the number of times it occurds.\n",
    "\n",
    "For example the string\n",
    "\n",
    "    \"The quick brown fox jumps over the lazy dog\"\n",
    "    \n",
    "would be transformed into\n",
    "\n",
    "| the | quick | brown | fox | jumps | over | lazy | dog |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "\n",
    "Note that:\n",
    "- the word \"the\" occurs twice so its count is 2\n",
    "- other words are unique so they only occur once\n",
    "- the number of features is the number of unique words\n",
    "\n",
    "Also note that for only one sentence we have 8 columns, as most of the words are distinct. If our analysis is focussed on a large *corpus* (or collection) of text documents, the number of columns is far larger. For instance the Google n-Grams corpus of publicly accessible web pages contains about one million distinct words.\n",
    "\n",
    "So text analytics suffers from the *high dimensionality* problem, wherein the number of columns in a dataset is large relative to the number of rows. Fitting models in a high-dimensional space means that there is a lot of room for error, so there is a large focus on reducing the dimensionality of textual data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TF-IDF\n",
    "\n",
    "In text data there will be lots of repeated words such as \"a\", \"is\" and \"the\" that aren't very useful, yet with BoW representation they will have a high associated weight. We should ignore them as much as possible.\n",
    "\n",
    "The Term Frequency–Inverse Document Frequency (TF-IDF) is a weighting procedure for BoW data. The TF-IDF weights boost the counts or frequency of uncommon words (which will be useful) and shrinks the mangitude associated with common words. There are two components to the TF-IDF weights, and each of these can be calculated in different ways:\n",
    "\n",
    "- Term Frequency, often the _raw count_ of a term in a document $tf = f_D$. Other possibilities are boolean (1 if the term appears, otherwise 0), length adjusted ($tf = \\frac{f_D}{n_{words}}$) or logarithmic ($tf = \\log(1+f_D)$).\n",
    "\n",
    "- Inverse Document Frequency, or a measure of the information contained in a word. This is a penalty for commonly used words like 'a' and 'the'. It's the logarithmically scaled inverse fraction of documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient). $idf = \\log(\\frac{N}{n_D})$ where $N$ is the number of documents and $n_D$ is the number of documents in which the word appears.\n",
    "\n",
    "The tfidf score is calculated as follows:\n",
    "\n",
    "$$tfidf = tf \\cdot idf $$\n",
    "\n",
    "As an example, consider the following corpus of two documents:\n",
    "\n",
    "    \"The quick brown fox jumps over the lazy dog\"\n",
    "    \"The five boxing wizards jump quickly\"\n",
    "    \n",
    "The *raw count* $tf$ weight of the word `The` is 2 in the first document, and 1 in the second. The $idf$ weight of `The`, defined for the whole corpus, is $idf = \\log(\\frac{2}{2}) = 0$. Hence the $tfidf$ weights of `The` in both documents is 0. This is appropriate since the word `The` carries very little meaning in each sentence.\n",
    "\n",
    "Now consider the word `wizards`. The raw count $tf$ weight is 0 in the first document, and 1 in the second. The $idf$ weight is $idf = \\log(\\frac{2}{1}) \\approx 0.7 $, the largest possible value in this corpus. Hence the $tfidf$ weight is 0 in the first document, and $tfidf \\approx 0.7$ in the second, which is appropriate since, intuitively, `wizard` contributes a lot of meaning to the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Topic models are statistical models that examine words from a set of documents, determine the themes over the text, and discover how the themes are associated or change over time. The process of topic modeling can be simplified to the following.\n",
    "\n",
    "1. Uncover the hidden topical patterns within a corpus.\n",
    "\n",
    "2. Annotate documents according to these topics.\n",
    "\n",
    "3. Use annotations to organize, search, and summarise texts.\n",
    "\n",
    "A topic is formally defined as a distribution over a fixed vocabulary of words. Different topics would have different distributions over the same vocabulary. A topic can be viewed as a cluster of words with related meanings, and each word has a corresponding weight inside this topic. Note that a word from the vocabulary can reside in multiple topics with different weights. Topic models do not necessarily require prior knowledge of the texts. The topics can emerge solely based on analyzing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "Loading required package: reshape2\n",
      "Loading required package: lda\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "“there is no package called ‘lda’”Warning message in data(cora.documents):\n",
      "“data set ‘cora.documents’ not found”Warning message in data(cora.vocab):\n",
      "“data set ‘cora.vocab’ not found”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in lda.collapsed.gibbs.sampler(cora.documents, K, 25, 0.1, 0.1, : could not find function \"lda.collapsed.gibbs.sampler\"\n",
     "output_type": "error",
     "traceback": [
      "Error in lda.collapsed.gibbs.sampler(cora.documents, K, 25, 0.1, 0.1, : could not find function \"lda.collapsed.gibbs.sampler\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "require(\"ggplot2\")\n",
    "require(\"reshape2\")\n",
    "require(\"lda\")\n",
    "\n",
    "# load documents and vocabulary\n",
    "data(cora.documents)\n",
    "data(cora.vocab)\n",
    "theme_set(theme_bw())\n",
    "\n",
    "# Number of topic clusters to display\n",
    "K <- 10\n",
    "# Number of documents to display\n",
    "N <- 9\n",
    "\n",
    "result <- lda.collapsed.gibbs.sampler(cora.documents,\n",
    "                                      K, ## Num clusters cora.vocab,\n",
    "                                      25, ## Num iterations\n",
    "                                      0.1,\n",
    "                                      0.1,\n",
    "                                      compute.log.likelihood=TRUE)\n",
    "\n",
    "# Get the top words in the cluster\n",
    "top.words <- top.topic.words(result$topics, 5, by.score=TRUE)\n",
    "\n",
    "# build topic proportions\n",
    "topic.props <- t(result$document_sums) / colSums(result$document_sums)\n",
    "\n",
    "document.samples <- sample(1:dim(topic.props)[1], N)\n",
    "\n",
    "topic.props <- topic.props[document.samples,]\n",
    "\n",
    "topic.props[is.na(topic.props)] <- 1 / K\n",
    "\n",
    "colnames(topic.props) <- apply(top.words, 2, paste, collapse=\" \")\n",
    "topic.props.df <- melt(cbind(data.frame(topic.props),\n",
    "                             document=factor(1:N)),\n",
    "                       variable.name=\"topic\",\n",
    "                       d.vars = \"document\")\n",
    "\n",
    "qplot(topic, value*100, fill=topic, stat=\"identity\",\n",
    "      ylab=\"proportion (%)\", data=topic.props.df,\n",
    "      geom=\"histogram\") +\n",
    "theme(axis.text.x = element_text(angle=0, hjust=1, size=12)) +\n",
    "coord_flip() +\n",
    "facet_wrap(~ document, ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
