{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Regression\n",
    "\n",
    "Regression analysis attempts to identify a relationship between a set of variables and some continuous outcome value. The outcome value is typically called the **dependent variable**, since it depends on the other variables, while these additional variables are called **independent variables** or **input variables**. Use cases of regression analysis include:\n",
    "\n",
    "**Real Estate**: Regression can be used to model the price of a home based on characteristics like living area. The model can then be used to predict the price of homes, and can be improved by including information about other characteristics like number of bathrooms, number of bedrooms, etc.\n",
    "    \n",
    "**Demand Forecasting**: Businesses and governments can use regression to predict demand for goods and services. For instance, a restaurant could use weather and date information to predict demand for certain dishes.\n",
    "    \n",
    "**Medical**: A linear regression model can be used to analyse the effect of a proposed radiation treatment on reducing tumor sizes. The input variables might include the duration of a single radiation treatment, frequency of radiation and patient attributes such as age or weight.\n",
    "\n",
    "## 5.1 Label, Features and Training Examples\n",
    "\n",
    "The **label** is the variable which we are trying to estimate. This could be a house price, weather temperature or even a categorical label like brand of car. Usually we use the letter $y$ to denote the label variable. It is also known as the dependant variable or target variable.\n",
    "\n",
    "A **feature** is an input variable which we use to estimate a label. We can have one or many features for each label. We use the letter $x$ to denote features. Some examples of features could be the size of a house, the number of bedrooms, distance to a school etc. When we have many features for a label we can write them as a vector.\n",
    "$$ \\mathbf x  = [x_1, x_2, ..., x_p] $$\n",
    "\n",
    "An **example** or **observation** is a pairing of label and feature. We use training examples to train a model. In other words, given a set of data features $\\mathbf x$ we adjust our model so that we get an accurate prediction for $y$.\n",
    "$$ [x_1, x_2, ..., x_p] \\rightarrow y $$\n",
    "\n",
    "An example of features and target for the house price example might include Square Feet, Bedrooms and Bathrooms as Features and the Price as the label.\n",
    "\n",
    "<img src=\"img/feature_target_table.png\" width=640>\n",
    "\n",
    "## 5.2 Training and Inference\n",
    "\n",
    "Once we have determined what the target variable and feature variables are our next step is to build and test a model. There are two main stages to this process:\n",
    "\n",
    "- **Training** refers to the learning process. We give the model training examples and the model corrects itself to model or learn the relationships between the features and label.\n",
    "- **Inference** is the process of applying our model to new data. You use the trained model to make predictions.\n",
    "\n",
    "<img src=\"img/training_inference.png\" width=840>\n",
    "\n",
    "\n",
    "## 5.3 Linear Regression\n",
    "\n",
    "A regression model means a model that predicts a label that is continuous based on a set of features. House prices and temperatures are examples of continuous variables that can be modeled using regression. Variables like brand of car or blood type are not continuous and can be desribed as discrete.\n",
    "\n",
    "Generally there are two types of linear regression:\n",
    "\n",
    "**Simple Linear Regression**\n",
    "$$ y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "This is also the general equation for a line in the $x-y$ plane. $y$ is the outcome variable and $x$ is a single input or feature variable. $\\beta_0$ is the intercept, or the $y$ value where the line meets the y-axis ($x=0$). $\\beta_1$ is the slope of the line (the tangent of the angle of the line with the x-axis), or equivalently, the change in $y$ induced by a unit increment of the feature variable $x$. A visualisation of simple linear regression is shown below. The model is a line in the $x-y$ plane.\n",
    "\n",
    "<img src=\"img/regression.png\" width=400>\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p$$\n",
    "\n",
    "This is the general equation for a hyperplane in the $p+1$ dimensional space spanned by $y-x_p$. $x_p$ are the input or feature variables, while $\\beta_p$ represent the slope of the hyperplane along the $x_p$ axes. A visualisation of multiple linear regression with two feature variables is shown below. The model is a plane in 3D $(x_1,x_2,y)$ space.\n",
    "\n",
    "<img src=\"img/multiple_regression.jpg\" width=250>\n",
    "\n",
    "A regression model can be uniquely described by the parameters $\\boldsymbol \\beta$. They can be interpreted as the effect or weight that each feature has on the value of the target, keeping all other features constant. Training a regression model is the process of finding the best $\\boldsymbol \\beta$ parameters to minimise the error between training target and predicted target values.\n",
    "\n",
    "**Simple** means that we only use one feature in this process, whereas **Multiple** regression means that we use multiple features in the model to explain the target variable.\n",
    "\n",
    "## 5.4 Ordinary Least Squares method\n",
    "\n",
    "The method of Ordinary Least Squares (OLS) chooses the optimal parameters of a linear model by minimising the squares of the differences between target values and predicted values. In other words, the OLS method attempts to minimise the sum of the squares of the vertical distances (residuals) shown by the dotted lines below.\n",
    "\n",
    "<img src=\"img/residuals.png\" width=350>\n",
    "\n",
    "The process of identifying these values is referred to as **training**.\n",
    "\n",
    "For a true target value $y_t$, we can write:\n",
    "$$ y_t = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p + e $$\n",
    "\n",
    "If we define the residuals $e$ as the difference between the target and the predicted value:\n",
    "$$ e = y_t - \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p $$\n",
    "\n",
    "then our objective becomes to minimise the square of the residuals: \n",
    "$$e^2 = ( y_t - \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p ) ^ 2 $$\n",
    "\n",
    "for all $n$ training examples:\n",
    "\n",
    "$$ \\min_{\\beta_0, \\beta_1, \\dots, \\beta_p} \\sum_{0}^{n}( y_t - \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi} ) ^ 2 $$\n",
    "\n",
    "Under the assumption of normally distributed errors (see Normality Assumption below), the OLS method is equivalent to Maximum Likelihood Estimation.\n",
    "\n",
    "## 5.5 House Price Example\n",
    "\n",
    "Let's look at an example using house price data collected in Baton Rouge, Louisiana, US, from mid-2005.\n",
    "\n",
    "The following variables are included in the file BatonRouge.csv:\n",
    "\n",
    "- Price: The price the house sold for\n",
    "- Square Feet: The size of the house in square feet\n",
    "- Beds: The number of bedrooms in the house\n",
    "- Baths: The number of bathrooms in the house\n",
    "- Age: The age of the house in years\n",
    "- Pool: Does the house have a pool (yes/no)\n",
    "- Fireplace: Does the house have a fireplace (yes/no)\n",
    "- Waterfront: Is the house on the waterfront (yes/no)\n",
    "- Days on Market: How many days the house spent on the market before it was ultimately sold\n",
    "\n",
    "### 5.5.1 Framing the Problem\n",
    "\n",
    "Given the Baton Rouge data we can do many regressions depending on which variable we select as the target or features. However there are two guiding questions you can ask yourself:\n",
    "\n",
    "- Which variable can you estimate from the others? (what is the dependency order of variables)\n",
    "- What is your goal?\n",
    "\n",
    "In this example our goal is to estimate the house price. House price is the most obvious dependent variable. All others have only weak dependency on each other. For example we expect that a bigger house is more expensive. But a more expensive house is not always bigger (due to factors such as location).\n",
    "\n",
    "**Target**: Price\n",
    "\n",
    "**Features**: All other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Price</th><th scope=col>SQFT</th><th scope=col>Bedrooms</th><th scope=col>Baths</th><th scope=col>Age</th><th scope=col>Occupancy</th><th scope=col>Pool</th><th scope=col>Fireplace</th><th scope=col>Waterfront</th><th scope=col>DOM</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td> 66500</td><td> 741</td><td>1</td><td>1</td><td>18</td><td>1</td><td>1</td><td>1</td><td>0</td><td>  6</td></tr>\n",
       "\t<tr><th scope=row>1</th><td> 66000</td><td> 741</td><td>1</td><td>1</td><td>18</td><td>2</td><td>1</td><td>0</td><td>0</td><td> 23</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 68500</td><td> 790</td><td>1</td><td>1</td><td>18</td><td>1</td><td>0</td><td>1</td><td>0</td><td>  8</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>102000</td><td>2783</td><td>2</td><td>2</td><td>18</td><td>1</td><td>0</td><td>1</td><td>0</td><td> 50</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 54000</td><td>1165</td><td>2</td><td>1</td><td>35</td><td>2</td><td>0</td><td>0</td><td>0</td><td>190</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>143000</td><td>2331</td><td>2</td><td>2</td><td>25</td><td>1</td><td>0</td><td>1</td><td>0</td><td> 86</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 10\n",
       "\\begin{tabular}{r|llllllllll}\n",
       "  & Price & SQFT & Bedrooms & Baths & Age & Occupancy & Pool & Fireplace & Waterfront & DOM\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t0 &  66500 &  741 & 1 & 1 & 18 & 1 & 1 & 1 & 0 &   6\\\\\n",
       "\t1 &  66000 &  741 & 1 & 1 & 18 & 2 & 1 & 0 & 0 &  23\\\\\n",
       "\t2 &  68500 &  790 & 1 & 1 & 18 & 1 & 0 & 1 & 0 &   8\\\\\n",
       "\t3 & 102000 & 2783 & 2 & 2 & 18 & 1 & 0 & 1 & 0 &  50\\\\\n",
       "\t4 &  54000 & 1165 & 2 & 1 & 35 & 2 & 0 & 0 & 0 & 190\\\\\n",
       "\t5 & 143000 & 2331 & 2 & 2 & 25 & 1 & 0 & 1 & 0 &  86\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 10\n",
       "\n",
       "| <!--/--> | Price &lt;int&gt; | SQFT &lt;int&gt; | Bedrooms &lt;int&gt; | Baths &lt;int&gt; | Age &lt;int&gt; | Occupancy &lt;int&gt; | Pool &lt;int&gt; | Fireplace &lt;int&gt; | Waterfront &lt;int&gt; | DOM &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0 |  66500 |  741 | 1 | 1 | 18 | 1 | 1 | 1 | 0 |   6 |\n",
       "| 1 |  66000 |  741 | 1 | 1 | 18 | 2 | 1 | 0 | 0 |  23 |\n",
       "| 2 |  68500 |  790 | 1 | 1 | 18 | 1 | 0 | 1 | 0 |   8 |\n",
       "| 3 | 102000 | 2783 | 2 | 2 | 18 | 1 | 0 | 1 | 0 |  50 |\n",
       "| 4 |  54000 | 1165 | 2 | 1 | 35 | 2 | 0 | 0 | 0 | 190 |\n",
       "| 5 | 143000 | 2331 | 2 | 2 | 25 | 1 | 0 | 1 | 0 |  86 |\n",
       "\n"
      ],
      "text/plain": [
       "  Price  SQFT Bedrooms Baths Age Occupancy Pool Fireplace Waterfront DOM\n",
       "0  66500  741 1        1     18  1         1    1         0            6\n",
       "1  66000  741 1        1     18  2         1    0         0           23\n",
       "2  68500  790 1        1     18  1         0    1         0            8\n",
       "3 102000 2783 2        2     18  1         0    1         0           50\n",
       "4  54000 1165 2        1     35  2         0    0         0          190\n",
       "5 143000 2331 2        2     25  1         0    1         0           86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data. The first column is just a row index, so we use 'row.names = 1'\n",
    "br <- read.csv('BatonRouge.csv', row.names = 1)\n",
    "\n",
    "head(br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first identify which variable linearly correlates most strongly with the target variable `Price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Price</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>SQFT</dt>\n",
       "\t\t<dd>0.760689174082246</dd>\n",
       "\t<dt>Bedrooms</dt>\n",
       "\t\t<dd>0.455189274657574</dd>\n",
       "\t<dt>Baths</dt>\n",
       "\t\t<dd>0.645340796822376</dd>\n",
       "\t<dt>Age</dt>\n",
       "\t\t<dd>-0.208608933147585</dd>\n",
       "\t<dt>Occupancy</dt>\n",
       "\t\t<dd>-0.206715518816963</dd>\n",
       "\t<dt>Pool</dt>\n",
       "\t\t<dd>0.202268683895296</dd>\n",
       "\t<dt>Fireplace</dt>\n",
       "\t\t<dd>0.310625728163814</dd>\n",
       "\t<dt>Waterfront</dt>\n",
       "\t\t<dd>0.30993997011312</dd>\n",
       "\t<dt>DOM</dt>\n",
       "\t\t<dd>0.0789293381675005</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Price] 1\n",
       "\\item[SQFT] 0.760689174082246\n",
       "\\item[Bedrooms] 0.455189274657574\n",
       "\\item[Baths] 0.645340796822376\n",
       "\\item[Age] -0.208608933147585\n",
       "\\item[Occupancy] -0.206715518816963\n",
       "\\item[Pool] 0.202268683895296\n",
       "\\item[Fireplace] 0.310625728163814\n",
       "\\item[Waterfront] 0.30993997011312\n",
       "\\item[DOM] 0.0789293381675005\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Price\n",
       ":   1SQFT\n",
       ":   0.760689174082246Bedrooms\n",
       ":   0.455189274657574Baths\n",
       ":   0.645340796822376Age\n",
       ":   -0.208608933147585Occupancy\n",
       ":   -0.206715518816963Pool\n",
       ":   0.202268683895296Fireplace\n",
       ":   0.310625728163814Waterfront\n",
       ":   0.30993997011312DOM\n",
       ":   0.0789293381675005\n",
       "\n"
      ],
      "text/plain": [
       "      Price        SQFT    Bedrooms       Baths         Age   Occupancy \n",
       " 1.00000000  0.76068917  0.45518927  0.64534080 -0.20860893 -0.20671552 \n",
       "       Pool   Fireplace  Waterfront         DOM \n",
       " 0.20226868  0.31062573  0.30993997  0.07892934 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor(br)[,'Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the variable `SQFT` has the largest absolute correlation coefficient, so we will use `SQFT` as our feature or input variable. We can build a simple linear model using the inbuilt `lm()` function in R. To build a model of `Price` as a function of `SQFT` we can use the formula argument `Price ~ SQFT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Price ~ SQFT, data = br)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-366641  -31399   -1535   25601  932272 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -60861.462   6110.187  -9.961   <2e-16 ***\n",
       "SQFT            92.747      2.411  38.476   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 79820 on 1078 degrees of freedom\n",
       "Multiple R-squared:  0.5786,\tAdjusted R-squared:  0.5783 \n",
       "F-statistic:  1480 on 1 and 1078 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- lm(Price ~ SQFT, br)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 Evaluating the Model\n",
    "\n",
    "How can we quantify the ability of our model to reflect the data?\n",
    "\n",
    "#### Goodness of fit\n",
    "- Residual Standard Error: measures the total error (distance between the regression line and actual dependant variable value for each training data point, summed over all training data points).\n",
    "- $R^2$: The R-squared value is a measure of the amount of variance that the model explains. A value of 1 means that the model totally explains all variance. However in most cases this would be considered overfitting.\n",
    "\n",
    "#### Effect of variables\n",
    "- coefficient estimate: the magnitude of the coefficient/param value $\\beta$. The larger this value the more it contributes to shifting the value of the dependant variable. A large coefficient value means the variable is practically significant. However since variables are at different scale you should standardise your variables first so their scales are the same. Some variables can't be compared in this way e.g. numeric vs categorical.\n",
    "- coefficient p-value: this is the statistical significance of the parameter value being non-zero. This does not mean that the variable should neccesarily be included in the model. You must also check the coefficient value. Often variables should be left in the regression even if their p-value is high so that you can indicate that you have controlled for this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4 Multivariate Linear Model\n",
    "\n",
    "Our univariate model has an $R^2$ of 0.58 which is good but not great. We can try to improve the model by including all feature variables in the dataframe and increasing the information available. To use all the columns as predictors of `Price` in the `lm()` function, we use the formula `Price ~ .`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4 Multivariate Linear Model\n",
    "\n",
    "Our univariate model has an $R^2$ of 0.58 which is good but not great. We can try to improve the model by including all feature variables in the dataframe and increasing the information available. To use all the columns as predictors of `Price` in the `lm()` function, we use the formula `Price ~ .`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Price ~ ., data = br)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-312713  -32475   -3565   28717  882827 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -45422.638  15978.345  -2.843  0.00456 ** \n",
       "SQFT            85.772      3.884  22.082  < 2e-16 ***\n",
       "Bedrooms    -26022.178   4539.147  -5.733 1.28e-08 ***\n",
       "Baths        39929.412   5702.413   7.002 4.44e-12 ***\n",
       "Age           -432.338    143.270  -3.018  0.00261 ** \n",
       "Occupancy     7563.501   4214.943   1.794  0.07302 .  \n",
       "Pool         -1896.596   8740.356  -0.217  0.82826    \n",
       "Fireplace    -1923.867   5244.975  -0.367  0.71384    \n",
       "Waterfront   58101.965   9215.011   6.305 4.20e-10 ***\n",
       "DOM            -21.542     24.461  -0.881  0.37870    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 74720 on 1070 degrees of freedom\n",
       "Multiple R-squared:  0.6335,\tAdjusted R-squared:  0.6304 \n",
       "F-statistic: 205.5 on 9 and 1070 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- lm(Price ~ ., br)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new $R^2$ is 0.63, a considerable improvement on the univariate model.\n",
    "\n",
    "### 5.5.5 Interpretation of Model Coefficients\n",
    "\n",
    "The model coefficients correspond to the change in the target variable induced by a unit change of the corresponding feature variable, all else held constant. However, the coefficients cannot be used to compare the importance of variables since each *unit* is on a different scale. A good way of seeing this is that a *unit* change of one square foot is far less significant than a *unit* change in, for example, number of bedrooms. We can use scaling to change the units of our feature variables such that their distributions are centred at zero and have a standard deviation of one. This ensures that a *unit* change for each feature has approximately equal significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Price ~ ., data = br_scaled)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.5442 -0.2642 -0.0290  0.2336  7.1826 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -3.156e-16  1.850e-02   0.000  1.00000    \n",
       "SQFT         7.035e-01  3.186e-02  22.082  < 2e-16 ***\n",
       "Bedrooms    -1.502e-01  2.620e-02  -5.733 1.28e-08 ***\n",
       "Baths        1.988e-01  2.840e-02   7.002 4.44e-12 ***\n",
       "Age         -6.048e-02  2.004e-02  -3.018  0.00261 ** \n",
       "Occupancy    3.659e-02  2.039e-02   1.794  0.07302 .  \n",
       "Pool        -4.179e-03  1.926e-02  -0.217  0.82826    \n",
       "Fireplace   -7.767e-03  2.118e-02  -0.367  0.71384    \n",
       "Waterfront   1.224e-01  1.942e-02   6.305 4.20e-10 ***\n",
       "DOM         -1.663e-02  1.889e-02  -0.881  0.37870    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.6079 on 1070 degrees of freedom\n",
       "Multiple R-squared:  0.6335,\tAdjusted R-squared:  0.6304 \n",
       "F-statistic: 205.5 on 9 and 1070 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the data\n",
    "br_scaled <- as.data.frame(scale(br))\n",
    "# Build a linear model\n",
    "model <- lm(Price ~ ., br_scaled)\n",
    "# Inspect the results\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling makes it possible to say that `SQFT` has the greatest effect on Price, since in scaled units it has the largest associated coefficient $\\beta$. `Bedrooms`, `Baths` and `Waterfront` are close behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.6 Generating Predictions\n",
    "\n",
    "Let's select a random row from our feature set as an example of a new data point that we want to predict the price of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 1 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>SQFT</th><th scope=col>Bedrooms</th><th scope=col>Baths</th><th scope=col>Age</th><th scope=col>Occupancy</th><th scope=col>Pool</th><th scope=col>Fireplace</th><th scope=col>Waterfront</th><th scope=col>DOM</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>122</th><td>1843</td><td>3</td><td>2</td><td>8</td><td>2</td><td>0</td><td>1</td><td>0</td><td>5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 9\n",
       "\\begin{tabular}{r|lllllllll}\n",
       "  & SQFT & Bedrooms & Baths & Age & Occupancy & Pool & Fireplace & Waterfront & DOM\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t122 & 1843 & 3 & 2 & 8 & 2 & 0 & 1 & 0 & 5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 9\n",
       "\n",
       "| <!--/--> | SQFT &lt;int&gt; | Bedrooms &lt;int&gt; | Baths &lt;int&gt; | Age &lt;int&gt; | Occupancy &lt;int&gt; | Pool &lt;int&gt; | Fireplace &lt;int&gt; | Waterfront &lt;int&gt; | DOM &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 122 | 1843 | 3 | 2 | 8 | 2 | 0 | 1 | 0 | 5 |\n",
       "\n"
      ],
      "text/plain": [
       "    SQFT Bedrooms Baths Age Occupancy Pool Fireplace Waterfront DOM\n",
       "122 1843 3        2     8   2         0    1         0          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a random row\n",
    "new_home <- br[123,]\n",
    "# Remove the target column\n",
    "new_home <- subset(new_home, select=-c(Price))\n",
    "# Inspect the output\n",
    "new_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a prediction from our model for this set of features using the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>122:</strong> 1295.9573572143"
      ],
      "text/latex": [
       "\\textbf{122:} 1295.9573572143"
      ],
      "text/markdown": [
       "**122:** 1295.9573572143"
      ],
      "text/plain": [
       "     122 \n",
       "1295.957 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(model, new_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Normality Assumption\n",
    "\n",
    "A robust assumption in statistics, physics and engineering is that random noise is normally distributed. Applied to linear regression, this assumption states that the residuals and errors of a regression model are normally distributed about the regression line. We can visualise the normality assumption by plotting a histogram of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAYAAAD958/bAAAEGWlDQ1BrQ0dDb2xvclNwYWNl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",
      "text/plain": [
       "Plot with title “Histogram of model$residuals”"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(model$residuals, breaks=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution closely resembles a normal curve. The residuals are centred at 0 and are approximately symmetric about 0.\n",
    "\n",
    "Knowing the probability distribution of the errors allows us to compute the probability of observing a certain outcome $y$, or the confidence of a prediction $\\hat{y}$. We can imagine that at each point of our regression line, there exists a normal distribution centred on the line itself, with standard deviation equal to the standard deviation of the residuals. Observations are assumed to be drawn from this normal distribution. A good rule of thumb is that 95% of errors will fall within two standard deviations of the regression line.\n",
    "\n",
    "We can thus calculate the 95% confidence interval for the price of a home with the features given by `new_home`, again using the `predict()` function and the argument `interval='confidence'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 1 × 3 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>122</th><td>1295.957</td><td>1180.934</td><td>1410.98</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1 × 3 of type dbl\n",
       "\\begin{tabular}{r|lll}\n",
       "  & fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t122 & 1295.957 & 1180.934 & 1410.98\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1 × 3 of type dbl\n",
       "\n",
       "| <!--/--> | fit | lwr | upr |\n",
       "|---|---|---|---|\n",
       "| 122 | 1295.957 | 1180.934 | 1410.98 |\n",
       "\n"
      ],
      "text/plain": [
       "    fit      lwr      upr    \n",
       "122 1295.957 1180.934 1410.98"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(model, new_home, level=0.95, interval='confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the same as computing the confidence interval of a *prediction* generated by the linear model. We can compute this using the argument `interval='prediction'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 1 × 3 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>122</th><td>1295.957</td><td>1180.928</td><td>1410.987</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1 × 3 of type dbl\n",
       "\\begin{tabular}{r|lll}\n",
       "  & fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t122 & 1295.957 & 1180.928 & 1410.987\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1 × 3 of type dbl\n",
       "\n",
       "| <!--/--> | fit | lwr | upr |\n",
       "|---|---|---|---|\n",
       "| 122 | 1295.957 | 1180.928 | 1410.987 |\n",
       "\n"
      ],
      "text/plain": [
       "    fit      lwr      upr     \n",
       "122 1295.957 1180.928 1410.987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(model, new_home, level=0.95, interval='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Multicolinearity\n",
    "\n",
    "The OLS method for linear regression presented above gives a unique set of coefficients $\\boldsymbol \\beta$ as long as the columns of our dataset $X$ are linearly independent. This means that if two or more of our feature variables are highly linearly correlated, then the model parameters can be highly variable, and the coefficients of the linear model can no longer be interpreted.\n",
    "\n",
    "To see this, imagine two columns $x_1$ and $x_2$ that are linearly dependent and thus perfectly correlated. We can write this as:\n",
    "\n",
    "$$ cx_1 = x_2$$\n",
    "\n",
    "for some proportionality constant c. Thus we can rewrite the linear model as:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 cx_1 = \\beta_0 + (\\beta_1 + c\\beta_2) x_1$$\n",
    "\n",
    "This is a simple linear regression problem with parameter $\\beta_1+c\\beta_2$, the optimal value of which can be given by a range of $\\beta_1$ and $\\beta_2$ parameters. Each run of the OLS algorithm will produce different results.\n",
    "\n",
    "## 5.8 Categorical Variables\n",
    "\n",
    "Notice that some of our variables, such as `Pool` or `Fireplace` are categorical. Fortunately these variables are binary and each state can be represented by a 0 or 1. The correct way to deal with categorical variables that can take on $m$ distinct values is to add $m-1$ binary variables to the regression model. These variables are called dummy variables. Consider the variable `Age` below, with three distinct possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 3 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Age</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Young </td></tr>\n",
       "\t<tr><td>Old   </td></tr>\n",
       "\t<tr><td>Middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 3 × 1\n",
       "\\begin{tabular}{r|l}\n",
       " Age\\\\\n",
       " <fct>\\\\\n",
       "\\hline\n",
       "\t Young \\\\\n",
       "\t Old   \\\\\n",
       "\t Middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 3 × 1\n",
       "\n",
       "| Age &lt;fct&gt; |\n",
       "|---|\n",
       "| Young  |\n",
       "| Old    |\n",
       "| Middle |\n",
       "\n"
      ],
      "text/plain": [
       "  Age   \n",
       "1 Young \n",
       "2 Old   \n",
       "3 Middle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Age <- c('Young', 'Old', 'Middle')\n",
    "df <- as.data.frame(Age)\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file would have two columns added for these variables, representing the first two values of `Age`. If a person was `Young` then the column `Young` would have a 1 and all other columns would be 0.  We can create dummy variables using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 3 × 3 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>(Intercept)</th><th scope=col>AgeOld</th><th scope=col>AgeYoung</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 3 × 3 of type dbl\n",
       "\\begin{tabular}{r|lll}\n",
       " (Intercept) & AgeOld & AgeYoung\\\\\n",
       "\\hline\n",
       "\t 1 & 0 & 1\\\\\n",
       "\t 1 & 1 & 0\\\\\n",
       "\t 1 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 3 × 3 of type dbl\n",
       "\n",
       "| (Intercept) | AgeOld | AgeYoung |\n",
       "|---|---|---|\n",
       "| 1 | 0 | 1 |\n",
       "| 1 | 1 | 0 |\n",
       "| 1 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  (Intercept) AgeOld AgeYoung\n",
       "1 1           0      1       \n",
       "2 1           1      0       \n",
       "3 1           0      0       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummies = model.matrix(~Age)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a person whose age is `Middle`, the one value not explicitly stated in the model, would be identified by setting all `Age` columns to 0. In this representation, `Middle` is considered the reference case, and the regression coefficients of the other `Age` variables would represent the difference in the target variable between `Age = Middle` and the other `Age` values. We do not include a column for `Middle` since the information in `Middle` is not independent of the other columns, leading to issues with multicolinearity.\n",
    "\n",
    "The `Intercept` column of ones preserves the intercept term $\\beta_0$ in our model. For more, see Appendix 2.\n",
    "\n",
    "## 5.9 Polynomial Regression and Overfitting\n",
    "\n",
    "As the name suggests, these algorithms are very good at modeling linear functions, that is functions where each feature is multiplied by a constant weight $\\beta$. One downside of these models is that any nonlinearities in the relationship between features and target might be missed. Nonlinearities are terms in our model function where feature variables are multiplied together (e.g. $\\beta_{i} x_1 x_2$, also called an *interaction term*), or where a feature is raised to a power (e.g. $\\beta_{i} x_1^2$). A polynomial of order 3 is a polynomial where the highest power is 3, e.g:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_{2} x_1^2 + \\beta_{3} x_1^3$$\n",
    "\n",
    "We can easily account for nonlinearities in our model by creating new features which multiply two feature columns together (e.g. $x_3 = x_1x_2$) or raise a feature column to a power (e.g. $x_3 = x_1^2$). By introducing new columns in this way, this allows us to model any multivariate polynomial function using the Ordinary Least Squares method.\n",
    "\n",
    "An interesting feature of polynomials is that they can model any arbitrary function, and a polynomial of order n can pass through any n+1 points exactly. Which means a higher order polynomial will be the best model, right? No! This leads to a situation where, if our model is too complex, the regressor can *learn the noise* inherent in our data and fail to identify the *underlying* functional relationship. The model will fit to the training data almost perfectly, but will not *generalise* well to new data. This is referred to as *overfitting*.\n",
    "\n",
    "Typically, in order to assess the degree of overfitting in our model and avoid learning any bias in our training data, a separate set is used for testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Training and Test Sets\n",
    "\n",
    "To select the \"best\" model for a machine learning task we need to evaluate the performance of each model. The first step in this process is seperating the data into training and test sets. Usually we take 70% of our collected data as the training set and the remaining 30% as our test set.\n",
    "\n",
    "**Training Set**: This set is used to train our models on. In other words we estimate the parameters for the models using this data.\n",
    "\n",
    "**Test Set**: This set is used ONLY to estimate the future predictive performance of the chosen model. The test set should never be used for training in any way whatsoever. It needs to remain completely independent and unseen by the models.\n",
    "\n",
    "The data in the test set must be distributed the same as the training set and also the same as future data. You might get unlucky in splitting your data and select lots of outliers. This can be addressed by collecting more data.\n",
    "\n",
    "<img src=\"img/train_test.png\" width=500>\n",
    "\n",
    "### 5.10.1 Train-Test-Split in R\n",
    "\n",
    "We can perform a train-test-split in R using the sample function to generate a number of random row indices to select from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed to make your partition reproducible\n",
    "set.seed(123)\n",
    "train_ind <- sample(seq_len(nrow(df)), size = floor(0.75 * nrow(df)))\n",
    "\n",
    "train <- df[train_ind, ]\n",
    "test <- df[-train_ind, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Evaluation Using the Training Set\n",
    "\n",
    "Since the Test set is off limits for the purposes of training and evaluation we must use the remaining data, which is the training set.\n",
    "\n",
    "### 5.11.1 Holdout\n",
    "\n",
    "The Holdout method simply reserves a portion of the training data as a Validation Set. The models are trained on the training set and the model performance is compared on the Validation Set.\n",
    "\n",
    "<img src=\"img/holdout.png\" width=500>\n",
    "\n",
    "### 5.11.2 Cross Validation\n",
    "\n",
    "The Holdout method is limited by the fact that the validation set is quite small. This means the validation set has a low chance of reflecting the full distribution of future data.\n",
    "\n",
    "To resolve this we can use all the training data as validation data. We successively partition the training set into different training and validation pairs. Performance is evaluated on all pairs and a sumamry statistic of accuracy (mean or median) is calculated.\n",
    "\n",
    "Cross Validation is an excellent estimator of \"out of sample\" performance. There are many types of Cross Validation such as \"leave-p-out\", \"leave-one-out\" and \"k-fold\". We will focus on k-fold cross validation. K refers to the number of splits of data. In the figure below there are 5 folds.\n",
    "\n",
    "<img src=\"img/cross_val.png\" width=800>\n",
    "\n",
    "\n",
    "## 5.12 Tuning Hyper Parameters\n",
    "\n",
    "Some models/algorithms will have additional parameters called hyper parameters. Hyper parameters are set before the parameter estimation process. Some common examples are regularisation parameters (control the magnitude of parameter values), learning rate or step size (gradient descent) or the number of neurons in a layer of a neural network.\n",
    "\n",
    "The optimal value of these parameters is hard to guess so you should always use cross validation to find the best parameter values. \n",
    "\n",
    "### 5.12.1 Process\n",
    "\n",
    "1. For each model, tune hyper parameters using cross validation on training set\n",
    "2. Perform cross validation on training set using each model to see which has the best predictive performance on average\n",
    "3. Estimate selected model performance using test set\n",
    "\n",
    "For step 1 the process is independent for each model. For step 2 we must use the same process i.e. validation or cross validation splits so that we conduct a fair test.\n",
    "\n",
    "Cross Validation mirrors test performance (out of sample generalisation) so we expect that the model that performs best in CV will perform best in the test set. From the test set we get an unbiased estimate of the performance on future data.\n",
    "\n",
    "<img src=\"img/model_selection.png\" width=900>\n",
    "\n",
    "\n",
    "### 5.12.2 Why is Validation Required?\n",
    "\n",
    "Why is a Validation set required?\n",
    "\n",
    "1. Selecting a model based on training set performance leads to a model that does not generalise to unseen data. In other words it leads to overfitting or a biased model.\n",
    "2. Using the test set as a validation set means that we no longer have an independent set to evaluate out of sample performance. The test set becomes tainted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "**The following sections are not necessary, but give a formal solution to the OLS minimisation problem above. Those without a mathematical background can safely skip it.**\n",
    "\n",
    "## A.1: Linear Algebra Review\n",
    "\n",
    "Linear algebra is the area of maths that deals with vector spaces and linear mappings between those spaces. Vectors are just an ordered list of numbers or variables, so we can represent any data as a vector. Take for instance the table of housing data below:\n",
    "\n",
    "<img width=320 src=\"img/datatable.png\">\n",
    "\n",
    "Each row can be represented as a vector in the $ a,b,c $ space spanned by the basis vectors corresponding to the variables Area, Bathrooms and Price:\n",
    "\n",
    "$$ \\mathbf a  = [1, 0, 0] $$\n",
    "$$ \\mathbf b  = [0, 1, 0] $$\n",
    "$$ \\mathbf c  = [0, 0, 1] $$\n",
    "\n",
    "The 1st row can then be represented as a 3-dimensional vector by taking a weighted sum of the basis vectors:\n",
    "\n",
    "$$ \\mathbf x_1  = 350 \\mathbf a + 3 \\mathbf b + 1060000 \\mathbf c = [350, 3, 1060000] $$\n",
    "\n",
    "Similarly, each column of data can be represented as a vector in our _sample_ space:\n",
    "\n",
    "$$ \\mathbf A = \\begin{bmatrix}350\\\\200\\\\540\\\\275\\end{bmatrix} $$ \n",
    "\n",
    "There is no reason why we could not represent our table columns as row vectors and our table rows as column vectors, we only need to be consistent in our choices.\n",
    "\n",
    "Matrices are like vectors with an additional dimension, and can be thought of as a collection of vectors. Conversely, we can also think of vectors as lower-dimensional matrices. We can represent the data in the table above as a $4\\times3$ matrix:\n",
    "\n",
    "$$ \\mathbf X = \\begin{bmatrix}350 & 3 & 1060000\\\\200 & 2 & 620000 \\\\540 & 3 & 1490000\\\\275 & 3 & 900000\\end{bmatrix} $$\n",
    "\n",
    "We say this matrix is rectangular since 4 ≠ 3. Matrices with an equal number of rows and columns ($m \\times m$ matrices), are referred to as _square_ matrices. These matrices are especially useful in that they can be invertible. This will be useful when dealing with matrix equations, but for now it's enough to know that square matrices are somewhat nicer to deal with than non-square ones.\n",
    "\n",
    "Matrices are also commonly used to represent image data. For example, the entries in the below matrix represent the greyscale value of each pixel in the image.\n",
    "\n",
    "<img src=\"img/digit.gif\" width=300>\n",
    "\n",
    "## A.2: Linear Regression in Matrix Form\n",
    "\n",
    "The equation for a multivariate linear model of a target variable $y$ in terms of features $x_p$ is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p$$\n",
    "\n",
    "If we write the $\\beta_i$ coefficients and features $x_i$ in vector form:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\...\\end{bmatrix} \\qquad \\mathbf x = \\begin{bmatrix}1&x_1&x_2&...\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can write the equation as:\n",
    "\n",
    "$$ \\mathbf x \\beta = y $$\n",
    "\n",
    "Adding an initial 1 to the vector $\\mathbf x$ to multiply the $\\beta_0$ intercept.\n",
    "\n",
    "If we expand the $\\mathbf x$ vector into a matrix, with each row representing an observation in our dataset, we can write the set of linear equations\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} ... \\beta_p x_{1p}= y_1 $$\n",
    "$$ \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} ... \\beta_p x_{2p}= y_2 $$\n",
    "$$ \\beta_0 + \\beta_1 x_{31} + \\beta_2 x_{32} ... \\beta_p x_{3p}= y_3 $$\n",
    "$$ ... $$\n",
    "$$ \\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} ... \\beta_p x_{np}= y_n $$\n",
    "\n",
    "In compact matrix form:\n",
    "\n",
    "$$\\begin{bmatrix}1&x_{11}&x_{12}&...&x_{1p}\\\\1&x_{21}&x_{22}&...&x_{2p}\\\\ & &...\\\\1&x_{n1}&x_{n2}&...&x_{np}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\...\\\\\\beta_p\\end{bmatrix} = \\begin{bmatrix}y_1\\\\y_2\\\\...\\\\y_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## A.3: Closed-form Analytical Solution with Linear Algebra\n",
    "\n",
    "We can also solve the linear regression problem directly using linear algebra. The goal is to minimise the squared error between the ground truth and the predictions from the linear model, defined by the coefficients in $\\beta$. For a data matrix $X$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}1&x_{11}&x_{12}&...&x_{1p}\\\\1&x_{21}&x_{22}&...&x_{2p}\\\\ & &...\\\\1&x_{n1}&x_{n2}&...&x_{np}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we can express the squared error in vector form:\n",
    "\n",
    "$$ \\sum_i\\left(\\sum_j X_{ij} \\times \\beta_j - y_i\\right)^2 = || X\\beta - \\mathbf y ||^2$$\n",
    "\n",
    "Where $||x||$ denotes the Frobenius norm of $x$. This minimisation problem has a unique solution as long as the columns of $X$ are linearly *independent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum of this function we take the derivitive of it w.r.t. $\\boldsymbol \\beta$. Notice first of all that the summation operator $\\Sigma_i$ commutes with the differential operator $\\frac{\\partial}{\\partial \\boldsymbol \\beta}$ so we have:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\boldsymbol \\beta} || X\\beta - \\mathbf y ||^2 = \\sum_i \\frac{\\partial}{\\partial \\boldsymbol \\beta} \\left(\\sum_j X_{ij} \\times \\beta_j - y_i\\right)^2$$\n",
    "\n",
    "Using $\\frac{\\partial}{\\partial x} f(x)^2 = 2f(x)\\frac{\\partial f(x)}{\\partial x}$ we obtain:\n",
    "\n",
    "$$2 \\sum_i \\left(\\sum_j X_{ij} \\times \\beta_j - y_i\\right) \\frac{\\partial}{\\partial \\beta_k} \\left(\\sum_j X_{ij} \\times \\beta_j - y_i\\right)$$\n",
    "\n",
    "We can evaluate the differential and get:\n",
    "\n",
    "$$2 \\sum_i X_{ik} \\left( \\sum_j X_{ij} \\times \\beta_j - y_i\\right)$$\n",
    "\n",
    "Replace the sum over $j$ with a vector multiplication:\n",
    "\n",
    "$$2 \\sum_i X_{ik} \\left(X_{i} \\boldsymbol \\beta - y_i\\right)$$\n",
    "\n",
    "Replace the sum over $i$ with a vector multiplication:\n",
    "\n",
    "$$  2 (X^T_k X \\boldsymbol \\beta - X^T_k \\mathbf y)$$\n",
    "\n",
    "And finally write it as a vector of values over k:\n",
    "\n",
    "$$  2 (X^T X \\boldsymbol \\beta - X^T \\mathbf y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set this to zero to find the values of $\\boldsymbol \\beta$ that yield a gradient of zero\n",
    "\n",
    "$$  X^T X \\beta = X^T \\mathbf y $$\n",
    "\n",
    "Inverting the square matrix $ X^T X$, we can solve explicitly for the model parameters $\\beta$:\n",
    "\n",
    "$$ \\beta = \\left( X^T X\\right)^{-1} X^T \\mathbf y$$\n",
    "\n",
    "We can now evaluate the $\\beta$ coefficients directly in R!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 10 × 1 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>-45422.63757</td></tr>\n",
       "\t<tr><th scope=row>SQFT</th><td>    85.77200</td></tr>\n",
       "\t<tr><th scope=row>Bedrooms</th><td>-26022.17811</td></tr>\n",
       "\t<tr><th scope=row>Baths</th><td> 39929.41204</td></tr>\n",
       "\t<tr><th scope=row>Age</th><td>  -432.33832</td></tr>\n",
       "\t<tr><th scope=row>Occupancy</th><td>  7563.50149</td></tr>\n",
       "\t<tr><th scope=row>Pool</th><td> -1896.59638</td></tr>\n",
       "\t<tr><th scope=row>Fireplace</th><td> -1923.86741</td></tr>\n",
       "\t<tr><th scope=row>Waterfront</th><td> 58101.96540</td></tr>\n",
       "\t<tr><th scope=row>DOM</th><td>   -21.54198</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 10 × 1 of type dbl\n",
       "\\begin{tabular}{r|l}\n",
       "\t & -45422.63757\\\\\n",
       "\tSQFT &     85.77200\\\\\n",
       "\tBedrooms & -26022.17811\\\\\n",
       "\tBaths &  39929.41204\\\\\n",
       "\tAge &   -432.33832\\\\\n",
       "\tOccupancy &   7563.50149\\\\\n",
       "\tPool &  -1896.59638\\\\\n",
       "\tFireplace &  -1923.86741\\\\\n",
       "\tWaterfront &  58101.96540\\\\\n",
       "\tDOM &    -21.54198\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 10 × 1 of type dbl\n",
       "\n",
       "| <!----> | -45422.63757 |\n",
       "| SQFT |     85.77200 |\n",
       "| Bedrooms | -26022.17811 |\n",
       "| Baths |  39929.41204 |\n",
       "| Age |   -432.33832 |\n",
       "| Occupancy |   7563.50149 |\n",
       "| Pool |  -1896.59638 |\n",
       "| Fireplace |  -1923.86741 |\n",
       "| Waterfront |  58101.96540 |\n",
       "| DOM |    -21.54198 |\n",
       "\n"
      ],
      "text/plain": [
       "           [,1]        \n",
       "           -45422.63757\n",
       "SQFT           85.77200\n",
       "Bedrooms   -26022.17811\n",
       "Baths       39929.41204\n",
       "Age          -432.33832\n",
       "Occupancy    7563.50149\n",
       "Pool        -1896.59638\n",
       "Fireplace   -1923.86741\n",
       "Waterfront  58101.96540\n",
       "DOM           -21.54198"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a vector of y values\n",
    "y <- as.matrix(br$Price)\n",
    "\n",
    "# Create a matrix with the feature variables\n",
    "X <- as.matrix(br[,2:10])\n",
    "\n",
    "# Add a column of ones to represent the intercept term\n",
    "X <- cbind(matrix(data = rep(1,nrow(X)), nrow = nrow(X), ncol = 1), X)\n",
    "\n",
    "# Evaluate the expression for the beta coefficients explicitly\n",
    "solve(t(X) %*% X) %*% t(X) %*% y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are identical to the ones produced by the `lm()` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
